{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, label_mapping=None, max_length=256, is_train=True):\n",
    "        \"\"\"\n",
    "        :param df: DataFrame с данными\n",
    "        :param tokenizer: токенизатор Hugging Face\n",
    "        :param label_mapping: словарь mapping оригинальных cat_id -> числовой индекс (только для обучающего набора)\n",
    "        :param max_length: максимальная длина последовательности\n",
    "        :param is_train: True, если набор данных используется для обучения (с метками)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_train = is_train\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        text = row['text']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Убираем лишнее измерение\n",
    "        encoding = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        if self.is_train:\n",
    "            # Преобразуем оригинальный cat_id в числовую метку с помощью label_mapping\n",
    "            orig_label = str(row['label'])\n",
    "            if self.label_mapping is not None:\n",
    "                label = self.label_mapping.get(orig_label, -1)\n",
    "            else:\n",
    "                label = int(row['label'])\n",
    "            encoding['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"f1\": f1_score(p.label_ids, preds),\n",
    "    }\n",
    "def train_model(df,val_df):\n",
    "    \"\"\"\n",
    "    Обучает модель на размеченных данных из файла labeled_train.parquet.\n",
    "    Сохраняет модель, токенизатор и маппинг меток в директорию ./model.\n",
    "    \"\"\"\n",
    "\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "\n",
    "    unique_labels = sorted(df['label'].unique())\n",
    "    label2id = {str(cat): idx for idx, cat in enumerate(unique_labels)}\n",
    "    id2label = {idx: str(cat) for idx, cat in enumerate(unique_labels)}\n",
    "    \n",
    "\n",
    "    model_name = \"answerdotai/ModernBERT-base\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(unique_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    train_dataset = ProductDataset(df, tokenizer, label_mapping=label2id, max_length=512, is_train=True)\n",
    "    val_dataset = ProductDataset(val_df, tokenizer, label_mapping=label2id, max_length=512, is_train=True)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./model1\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        eval_strategy='steps',\n",
    "        eval_steps=250,\n",
    "        #save_strategy=\"epoch\",\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=500,\n",
    "        learning_rate=5e-5,\n",
    "        #save_only_model=1,\n",
    "        optim=\"adamw_torch_fused\", # improved optimizer \n",
    "        logging_strategy=\"steps\",\n",
    "        report_to=None  # отключаем логирование в WandB и т.п.\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Сохраняем модель и токенизатор\n",
    "    trainer.save_model(\"./model1\")\n",
    "    tokenizer.save_pretrained(\"./model1\")\n",
    "    \n",
    "    # Сохраняем mapping меток для последующего использования\n",
    "    mapping = {\"label2id\": label2id, \"id2label\": id2label}\n",
    "    with open(os.path.join(\"model1\", \"label_mapping.json\"), \"w\") as f:\n",
    "        json.dump(mapping, f)\n",
    "    \n",
    "    print(\"Модель обучена и сохранена в директорию ./model\")\n",
    "    return model, tokenizer, label2id, id2label, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "df = pd.read_csv(\"/kaggle/input/datafacts/train.csv\")\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return str(text).strip()\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "df = df[df['text'].str.len() > 0]\n",
    "train_df, val_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42,  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model, tokenizer, l2id, i2l, train_result = train_model(train_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "metrics = train_result.metrics\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "# Оценка на валидационном наборе\n",
    "val_metrics = trainer.evaluate()\n",
    "trainer.save_metrics(\"eval\", val_metrics)\n",
    "\n",
    "print(\"\\nРезультаты обучения:\")\n",
    "print(f\"Тренировочные потери: {metrics['train_loss']:.4f}\")\n",
    "print(f\"Валидационная точность: {val_metrics['eval_accuracy']:.4f}\")\n",
    "print(f\"Валидационный F1: {val_metrics['eval_f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
